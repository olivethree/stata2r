[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cookbook Data Analysis with Stata and R",
    "section": "",
    "text": "Welcome to the “Cookbook Data Analysis with Stata and R”! This book is designed to be your comprehensive guide to mastering data analysis using two of the most powerful tools available: Stata and R. Whether you are a beginner or have some experience, this book will help you develop the skills needed to tackle a wide range of data analysis challenges.\n\n\nIn the Human Technology Interaction track at TU/e, understanding and analyzing data is crucial. This book aims to provide you with practical, hands-on experience in data analysis, tailored specifically to the needs of your coursework and future career. By the end of this book, you will be able to:\n\nImport and manage data in both Stata and R.\nClean and prepare your data for analysis.\nPerform a variety of statistical analyses.\nVisualize your data to uncover insights.\nCommunicate your findings effectively.\n\n\n\n\nData analysis is a vast field, and this book focuses on giving you a solid foundation in the most essential tools and techniques. Here’s what you can expect to learn:\n\nData Import and Management: Learn how to import data from various sources and manage it efficiently in Stata and R.\nData Cleaning and Preparation: Understand the importance of tidy data and how to clean and prepare your datasets for analysis.\nStatistical Analysis: Perform descriptive and inferential statistics to draw meaningful conclusions from your data.\nData Visualization: Create compelling visualizations to explore and present your data.\nReproducible Research: Learn best practices for ensuring your analyses are reproducible and transparent.\n\n\n\n\nThroughout this book, you will work on simulated data sets (this means they are not based on real data) aiming to reflect real-world examples and case studies relevant to Human Technology Interaction. These examples will help you see how the techniques you learn can be applied to actual research and industry scenarios. Whether you are analyzing user behavior, evaluating the effectiveness of a new technology, or exploring human-computer interaction, this book will provide you with the most commonly used tools you need.\n\n\n\nTo get the most out of this book, you will need to have Stata and R installed on your computer. Familiarity with basic statistical concepts will be helpful, but not required, as we will cover the necessary background along the way."
  },
  {
    "objectID": "intro.html#r",
    "href": "intro.html#r",
    "title": "1  Introduction",
    "section": "1.2 R",
    "text": "1.2 R\nR is a programming language and free software environment for statistical computing and graphics. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and the first public announcement of R was made in 1993 1(Peng 2016). The language was inspired by the S language, which was developed at Bell Laboratories. R has since become a popular tool among statisticians and data scientists due to its extensive package ecosystem and active user community."
  },
  {
    "objectID": "intro.html#stata",
    "href": "intro.html#stata",
    "title": "1  Introduction",
    "section": "1.3 Stata",
    "text": "1.3 Stata\nStata is a general-purpose statistical software package created in the mid-1980s by William Gould, a UCLA graduate, and Sean Becketti 2(Cox 2005). Initially developed within the Computing Resource Center (CRC) in Santa Monica, California, Stata was designed to provide a powerful yet user-friendly environment for data analysis. The first version of Stata was released in 1985, and it has since evolved to include a wide range of statistical, graphical, and data management capabilities."
  },
  {
    "objectID": "intro.html#comparison-of-r-and-stata",
    "href": "intro.html#comparison-of-r-and-stata",
    "title": "1  Introduction",
    "section": "1.4 Comparison of R and Stata",
    "text": "1.4 Comparison of R and Stata\n\n\n\n\n\n\n\n\nFeature\nR\nStata\n\n\n\n\nCost\nFree and open-source\nPaid software\n\n\nUser Interface\nCommand-line interface, with various IDEs like RStudio available\nUser-friendly GUI and command-line interface\n\n\nFlexibility\nHighly flexible, suitable for custom analyses and new methods\nLess flexible, but highly efficient for standard statistical tasks\n\n\nPackages\nExtensive package ecosystem for various analyses and visualizations\nComprehensive built-in functions and user-written commands\n\n\nLearning Curve\nSteeper learning curve, especially for beginners\nEasier to learn, with simpler syntax\n\n\nCommunity Support\nLarge, active community with extensive online resources\nSmaller community, but excellent official documentation\n\n\nReproducibility\nStrong support for reproducible research through RMarkdown and other tools\nGood support for reproducibility, but less integrated than R\n\n\nData Management\nPowerful data manipulation capabilities with packages like dplyr\nEfficient data manipulation and management built-in\n\n\nVisualization\nAdvanced visualization capabilities with ggplot2 and other packages\nGood visualization tools, but less flexible than R\n\n\n\n\n\n\n\nCox, Nicholas J. 2005. “A Brief History of Stata on Its 20th Anniversary.” The Stata Journal: Promoting Communications on Statistics and Stata 5 (1): 2–18. https://doi.org/10.1177/1536867x0500500102.\n\n\nPeng, Roger D. 2016. R Programming for Data Science. Leanpub Victoria, BC, Canada."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "2  Getting Started",
    "section": "",
    "text": "This chapter provides a quick tutorial on how to install and set up R and Stata on both Windows and Mac computers. By the end of this chapter, you’ll have the necessary tools ready to begin your analysis."
  },
  {
    "objectID": "getting_started.html#installing-r",
    "href": "getting_started.html#installing-r",
    "title": "2  Getting Started",
    "section": "2.2 Installing R",
    "text": "2.2 Installing R\n\n2.2.1 Windows\n\nDownload R:\n\nGo to the R Project website.\nClick on “Download R for Windows.”\nClick on “base” to download the base R package.\n\nInstall R:\n\nRun the downloaded .exe file.\nFollow the installation instructions, accepting the default settings.\n\nInstall RStudio (Optional but recommended):\n\nDownload RStudio from the RStudio website.\nRun the installer and follow the setup instructions.\n\n\n\n\n2.2.2 Mac\n\nDownload R:\n\nVisit the R Project website.\nClick on “Download R for macOS.”\n\nInstall R:\n\nOpen the downloaded .pkg file.\nFollow the installation instructions.\n\nInstall RStudio (Optional but recommended):\n\nDownload RStudio from the RStudio website.\nOpen the .dmg file and drag RStudio to your Applications folder."
  },
  {
    "objectID": "getting_started.html#installing-stata",
    "href": "getting_started.html#installing-stata",
    "title": "2  Getting Started",
    "section": "2.3 Installing Stata",
    "text": "2.3 Installing Stata\n\n2.3.1 Windows\n\nObtain a License:\n\nStata is (unfortunately) commercial software. You need a valid license. Look it up in your course page or ask your teacher.\n\nDownload Stata:\n\nGo to the Stata website and log in to your account to download the installer.\n\nInstall Stata:\n\nRun the downloaded .exe file.\nFollow the installation instructions, entering your license information when asked for it.\n\n\n\n\n2.3.2 Mac\n\nObtain a License:\n\nStata is (unfortunately) commercial software. You need a valid license. Look it up in your course page or ask your teacher.\n\nDownload Stata:\n\nVisit the Stata website and log in to your account to download the installer.\n\nInstall Stata:\n\nOpen the downloaded .dmg file.\nDrag the Stata application to your Applications folder.\nLaunch Stata and enter your license information."
  },
  {
    "objectID": "getting_started.html#setting-up-your-environment",
    "href": "getting_started.html#setting-up-your-environment",
    "title": "2  Getting Started",
    "section": "2.4 Setting Up Your Environment",
    "text": "2.4 Setting Up Your Environment\n\n2.4.1 R Setup\n\nOpen RStudio (or R GUI if not using RStudio).\nInstall Essential Packages:\n\nOpen the Console and run:\n\n\n\n   install.packages(c(\"tidyverse\", \"magrittr\", \"here\"))\n\n\nCreate a New Project (Optional but recommended in RStudio):\n\nGo to “File” > “New Project” > “New Directory” > “New Project.”\nChoose a location and name for your project, then click “Create Project.”\n\n\n\n\n2.4.2 Stata Setup\n\nOpen Stata.\nSet a Working Directory:\n\nUse the command:\n\n\n   cd \"path/to/your/directory\"\n\nReplace \"path/to/your/directory\" with the path where you want to save your files.\nCreating Do-Files:\n\nGo to “File” > “New Do-file Editor.”\nSave the Do-file in your working directory."
  },
  {
    "objectID": "getting_started.html#verification",
    "href": "getting_started.html#verification",
    "title": "2  Getting Started",
    "section": "2.5 Verification",
    "text": "2.5 Verification\n\n2.5.1 R\n\nTest Installation:\n\nIn RStudio or R GUI, type:\n\n\n   print(\"R is working! Yay!\")\n\n\nIf you see the output [1] \"R is working!\", your installation is successful.\n\nLoad a Package:\n\nRun:\n\n\n   library(ggplot2)\n   print(\"Package 'ggplot2' is loaded!\")\n\n\n\n\n2.5.2 Stata\n\nTest Installation:\n\nIn the Command window, type:\n\n\n   display \"Stata is working! Yay!\"\n\n\nIf you see the output Stata is working!, your installation is successful.\n\nCheck Version:\n\nType:\n\n\n   about\n\n\nThis will display the version of Stata installed.\n\n\n\nWith your environment set up, you’re now ready to start performing analyses using R and Stata!"
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "3  t-test",
    "section": "",
    "text": "# Load the necessary packages \nlibrary(tidyverse) # used for data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(cowplot) # adds new plotting themes for data visualization\n\n\nAttaching package: 'cowplot'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nlibrary(effsize) # or effect size calculations\n\n# to install any missing packages go to the Terminal and run the command: install.packages(\"PACKAGE_NAME\")"
  },
  {
    "objectID": "ttest.html#brief-explanation",
    "href": "ttest.html#brief-explanation",
    "title": "3  t-test",
    "section": "3.2 Brief Explanation",
    "text": "3.2 Brief Explanation\nThe t-test, proposed by William Sealy Gosset under the pseudonym “Student” in 1908, is used to determine if there is a significant difference between the means of two groups. The t-test is applicable in various scenarios, including both small and large sample sizes, particularly when the population variance is unknown. The t-test assumes that the data is approximately normally distributed.\nThere are two main types of t-tests commonly used:\n\nIndependent Samples t-test: Compares the means of two independent groups (between-subjects design).\nPaired Samples t-test: Compares means from the same group at different times or under different conditions (within-subjects design).\n\n\n3.2.1 Understanding Independent and Dependent Data\n\nIndependent Data: In an independent samples t-test, the data from the two groups are independent, meaning that there is no inherent relationship between the observations in one group and the observations in the other group. This is typical of a between-subjects design, where different participants are assigned to different conditions.\nDependent Data: In a paired samples t-test, the data are dependent, meaning that each observation in one condition is paired with an observation in another condition. This is typical of a within-subjects design, where the same participants are measured under different conditions or at different times."
  },
  {
    "objectID": "ttest.html#research-scenario",
    "href": "ttest.html#research-scenario",
    "title": "3  t-test",
    "section": "3.3 Research Scenario",
    "text": "3.3 Research Scenario\nImagine a tech company, “InnovateTech,” which has recently launched a new interface design for its flagship software. The company is keen to understand whether this new design truly enhances user satisfaction compared to the old design. The company has two different research questions:\n\nIndependent Samples Scenario: InnovateTech randomly assigns users to use either the old or the new design and then measures their satisfaction. The research question is: Does the new interface design improve user satisfaction compared to the old design?\nPaired Samples Scenario: InnovateTech asks the same users to use both the old and new designs at different times and then measures their satisfaction. The research question is: Does user satisfaction improve after using the new interface design compared to the old design?"
  },
  {
    "objectID": "ttest.html#independent-samples-t-test",
    "href": "ttest.html#independent-samples-t-test",
    "title": "3  t-test",
    "section": "3.4 Independent Samples t-test",
    "text": "3.4 Independent Samples t-test\n\n3.4.1 Research Question\nResearch Question: Does the new interface design improve user satisfaction compared to the old design?\nHypothesis: - Null Hypothesis (H₀): There is no significant difference in user satisfaction scores between the old and new interface designs. - Alternative Hypothesis (H₁): Users report higher satisfaction scores with the new interface design compared to the old design.\n\n\n3.4.2 Simulated Dataset\n\n3.4.2.1 Stata\n\nclear\nset seed 123\nset obs 60\ngen group = cond(_n <= 30, \"Old Design\", \"New Design\")\ngen satisfaction = rnormal(70 + 5 * (group == \"New Design\"), 10)\nsave satisfaction_data_independent.dta, replace\n\n\n\n3.4.2.2 R\n\nset.seed(123)\nn <- 30\nold_design <- rnorm(n, mean = 70, sd = 10)\nnew_design <- rnorm(n, mean = 75, sd = 10)\nstudydata <- data.frame(\n  group = rep(c(\"Old Design\", \"New Design\"), each = n),\n  satisfaction = c(old_design, new_design)\n)\nwrite.csv(studydata, \"satisfaction_data_independent.csv\", row.names = FALSE)\n\n\n\n\n3.4.3 Descriptives\n\n3.4.3.1 Stata\n\nuse satisfaction_data_independent.dta\nsummarize satisfaction\ngraph box satisfaction, over(group)\n\n\n\n3.4.3.2 R\n\nstudydata <- read.csv(\"satisfaction_data_independent.csv\")\nsummary(studydata$satisfaction)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  50.33   65.48   73.26   73.16   80.62   96.69 \n\np <- ggplot(studydata, aes(x = group, y = satisfaction)) +\n  geom_boxplot() +\n  theme_cowplot() +\n  labs(title = \"Satisfaction Scores by Group\", x = \"Group\", y = \"Satisfaction Score\")\np\n\n\n\n\n\n\n\n3.4.4 Performing the t-test\n\n3.4.4.1 Important Note About R’s t-test\nBy default, R uses Welch’s t-test, which does not assume equal variances between the groups. This is often a more robust approach, but if you want to match the classical t-test calculation (assuming equal variances), you need to specify var.equal = TRUE in the t.test() function. The code provided here uses the classical approach to ensure it matches the Stata output.\n\n\n3.4.4.2 Stata\n\nuse satisfaction_data_independent.dta\nttest satisfaction, by(group)\n\n\n\n3.4.4.3 R\n\nt.test(satisfaction ~ group, data = studydata, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  satisfaction by group\nt = 3.0841, df = 58, p-value = 0.003125\nalternative hypothesis: true difference in means between group New Design and group Old Design is not equal to 0\n95 percent confidence interval:\n  2.545972 11.962870\nsample estimates:\nmean in group New Design mean in group Old Design \n                76.78338                 69.52896 \n\n\n\n\n\n3.4.5 Interpretation\n\nP-value: Indicates whether the difference in means is statistically significant.\nConfidence Interval: By default, this is a 95% confidence interval, but you can adjust this with the conf.level parameter in the t.test() function."
  },
  {
    "objectID": "ttest.html#paired-samples-t-test",
    "href": "ttest.html#paired-samples-t-test",
    "title": "3  t-test",
    "section": "3.5 Paired Samples t-test",
    "text": "3.5 Paired Samples t-test\n\n3.5.1 Research Question\nResearch Question: Does user satisfaction improve after using the new interface design compared to the old design?\nHypothesis: - Null Hypothesis (H₀): There is no significant difference in user satisfaction scores before and after using the new interface design. - Alternative Hypothesis (H₁): Users report higher satisfaction scores after using the new interface design compared to before.\n\n\n3.5.2 Simulated Dataset\n\n3.5.2.1 Stata\n\nclear\nset seed 123\nset obs 30\ngen user_id = _n\ngen before_update = rnormal(70, 10)\ngen after_update = rnormal(75, 10)\nsave satisfaction_data_paired.dta, replace\n\n\n\n3.5.2.2 R\n\nset.seed(123)\nn <- 30\nbefore_update <- rnorm(n, mean = 70, sd = 10)\nafter_update <- rnorm(n, mean = 75, sd = 10)\nstudydata <- data.frame(\n  user_id = 1:n,\n  before_update = before_update,\n  after_update = after_update\n)\nwrite.csv(studydata, \"satisfaction_data_paired.csv\", row.names = FALSE)\n\n\n\n\n3.5.3 Descriptives\n\n3.5.3.1 Stata\n\nuse satisfaction_data_paired.dta\nsummarize before_update after_update\ngraph box before_update after_update, names(b1 b2)\n\n\n\n3.5.3.2 R\n\nstudydata <- read.csv(\"satisfaction_data_paired.csv\")\nsummary(studydata[, c(\"before_update\", \"after_update\")])\n\n before_update    after_update  \n Min.   :50.33   Min.   :59.51  \n 1st Qu.:63.29   1st Qu.:71.97  \n Median :69.26   Median :75.48  \n Mean   :69.53   Mean   :76.78  \n 3rd Qu.:74.89   3rd Qu.:82.57  \n Max.   :87.87   Max.   :96.69  \n\n# Reshape the data to long format\nstudydata_long <- studydata %>%\n  pivot_longer(cols = c(\"before_update\", \"after_update\"), \n               names_to = \"Condition\", \n               values_to = \"Satisfaction\")\n\n# Create the boxplot\np <- ggplot(studydata_long, aes(x = Condition, y = Satisfaction, fill = Condition)) +\n  geom_boxplot(alpha = 0.7) +\n  theme_cowplot() +\n  labs(title = \"Satisfaction Scores Before and After Update\", \n       x = \"Condition\", \n       y = \"Satisfaction Score\") +\n  scale_fill_manual(values = c(\"blue\", \"green\"))\n\np\n\n\n\n\n\n\n\n3.5.4 Performing the t-test\n\n3.5.4.1 Stata\n\nuse satisfaction_data_paired.dta\nttest before_update == after_update\n\n\n\n3.5.4.2 R\n\nstudydata <- read.csv(\"satisfaction_data_paired.csv\")\nt.test(studydata$before_update, studydata$after_update, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  studydata$before_update and studydata$after_update\nt = -2.8692, df = 29, p-value = 0.007601\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -12.425597  -2.083245\nsample estimates:\nmean difference \n      -7.254421 \n\n\n\n\n\n3.5.5 Interpretation\n\nP-value: Indicates whether the difference in means is statistically significant.\nConfidence Interval: It can be adjusted from the default 95% to another level if needed (but simply use 95% in case you are in doubt)."
  },
  {
    "objectID": "ttest.html#explanation-of-relevant-terms",
    "href": "ttest.html#explanation-of-relevant-terms",
    "title": "3  t-test",
    "section": "3.6 Explanation of Relevant Terms",
    "text": "3.6 Explanation of Relevant Terms\n\n\n\n\n\n\n\n\nTerm\nDefinition\nCommon Misconception\n\n\n\n\nP-value\nThe probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is true.\nThe p-value is the probability that the null hypothesis is true.\n\n\nConfidence Interval\nA range of values, derived from the sample data, that is believed to contain the true parameter value with a certain probability. The most common level of confidence is 95%, but this can be adjusted (e.g., to 90% or 99%) depending on the analysis.\nA 95% confidence interval means there is a 95% probability that the true parameter lies within the interval.\n\n\nT-statistic\nA ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. The degrees of freedom (df) are the number of independent values that can vary in an analysis without breaking any constraints.\nThe t-statistic directly tells us the probability of the null hypothesis being true."
  },
  {
    "objectID": "ttest.html#interpretation-questions",
    "href": "ttest.html#interpretation-questions",
    "title": "3  t-test",
    "section": "3.7 Interpretation Questions",
    "text": "3.7 Interpretation Questions\n\nWhat does a significant p-value indicate in the context of this t-test?\n\n\nShow/Hide Solution 1\n\n\nSolution 1: A significant p-value indicates that the observed data is unlikely under the null hypothesis. This suggests that there is evidence against the null hypothesis, implying a statistically significant difference between the satisfaction scores of the two groups (for independent samples) or before and after the update (for paired samples). However, it does not measure the probability that the null hypothesis is true or false.\n\n\nHow would you interpret the confidence interval in this analysis?\n\n\nShow/Hide Solution 2\n\n\nSolution 2: The confidence interval provides a range of values that, based on the sample data, is likely to contain the true mean difference. If we were to repeat the experiment many times, we would expect a specified proportion (e.g., 95%) of these intervals to contain the true mean difference. It does not mean that there is a 95% probability that the true mean difference lies within this specific interval. The most common confidence level is 95%, but it can be adjusted depending on the requirements of the analysis."
  },
  {
    "objectID": "ttest.html#effect-sizes-for-t-tests",
    "href": "ttest.html#effect-sizes-for-t-tests",
    "title": "3  t-test",
    "section": "3.8 Effect Sizes for t-tests",
    "text": "3.8 Effect Sizes for t-tests\nEffect sizes are a crucial part of reporting t-test results because they provide information on the magnitude of the difference between groups or conditions, beyond just the statistical significance. Below, we discuss the common effect sizes to report for Independent Samples t-tests and Paired Samples t-tests.\nThe interpretation guidelines for effect sizes such as Cohen’s d and the correlation coefficient (r) are based on widely accepted conventions in the field of psychology and social sciences. These conventions were originally proposed by Jacob Cohen in his foundational work on statistical power analysis.\nA common cautionary quote from Jacob Cohen regarding his proposed guidelines on effect sizes can be found in his book Statistical Power Analysis for the Behavioral Sciences (Cohen 1988) 1. Cohen emphasized that the guidelines he provided for interpreting effect sizes (i.e., small, medium, and large) were meant to be rough, arbitrary conventions rather than rigid rules. Here’s a frequently cited quote:\n\n“The terms ‘small,’ ‘medium,’ and ‘large’ are relative, not only to each other but to the area of behavioral science or even more particularly to the specific content and research method being employed. A medium effect in one area may be considered small in another, or even large in a third. The conventions were chosen as recommended operational definitions, because they are reasonable and because they are frequently used as conventional values in the social sciences, but they are not to be applied mechanically or universally” (Cohen, 1988, p. 25).\n\nThis quote underscores that the interpretation of effect sizes should be context-dependent and that researchers should avoid applying these guidelines too rigidly. Instead, Cohen urged that the significance of an effect size should be interpreted in the context of the specific research question, field of study, and methodology used.\n\n3.8.0.1 Independent t-test\nFor an Independent Samples t-test, the most common effect size to report is Cohen’s d.\n\n\n3.8.1 Cohen’s d Calculation:\n\nCohen’s (d) is calculated as the difference between two means divided by the pooled standard deviation of the groups. The formula is:\n\n[ d = ]\nWhere:\n\n\\[\\bar{X}_1\\] = the mean of the first group\n\\[\\bar{X}_2\\] = the mean of the second group\n\\[s_1\\] = the standard deviation of the first group\n\\[s_2\\] = the standard deviation of the second group\n\\[n_1\\] = the sample size of the first group\n\\[n_2\\] = the sample size of the second group\n\nCalculating Cohen’s d for Independent Samples t-test:\n\nR:\n\nTo calculate Cohen’s d for an Independent Samples t-test in R, you can use the effsize package, which provides a reliable implementation.\n\n# We need to use the dataset with the independent groups\n# Regenerating the dataset to make sure we have the right one\nset.seed(123)\nn <- 30\nold_design <- rnorm(n, mean = 70, sd = 10)\nnew_design <- rnorm(n, mean = 75, sd = 10)\nstudydata_independent <- data.frame(\n  group = rep(c(\"Old Design\", \"New Design\"), each = n),\n  satisfaction = c(old_design, new_design)\n)\n\n\n# Note: Use the dataset with independent samples: studydata_long\n\n# Compute cohen's d\ncohen_d <- cohen.d(satisfaction ~ group, data = studydata_independent, pooled = TRUE)\nprint(cohen_d)\n\n\nCohen's d\n\nd estimate: 0.7963098 (medium)\n95 percent confidence interval:\n    lower     upper \n0.2593757 1.3332438 \n\n\n\nStata:\n\nStata doesn’t have a built-in command to directly calculate Cohen’s d, but it can be calculated using the following steps:\n\n* Make sure the appropriate dataset is loaded (e.g. use DATASET_NAME)\n* Assuming the data is already loaded in Stata\nttest satisfaction, by(group)\n\n* Calculate manually\ngen pooled_sd = sqrt(((r(sd_1)^2) * (r(N_1) - 1) + (r(sd_2)^2) * (r(N_2) - 1)) / (r(N_1) + r(N_2) - 2))\ngen cohen_d = (r(mu_1) - r(mu_2)) / pooled_sd\ndisplay cohen_d\n\n\n\n\n\n\n\n\n\nEffect Size\nRequired Information\nHow to Find It in Articles\n\n\n\n\nCohen’s d\nMeans of both groups, standard deviations, sample sizes\nLook for mean differences, standard deviations, and sample sizes in the results section of articles. Typically, these are presented in tables or described in the text.\n\n\n\n\n3.8.1.1 Paired t-test\nFor a Paired Samples t-test, the common effect sizes to report include Cohen’s d for paired samples and Correlation coefficient (r).\nIn paired samples, Cohen’s d is calculated using the formula:\n\\[\nd = \\frac{\\bar{X}_D}{s_D}\n\\]\nWhere:\n\n\\[\\bar{X}\\_D\\] = mean difference \\[s_D\\] = standard deviation of the difference\n\n\n\n\n3.8.2 Correlation Coefficient (r) Calculation:\nThe correlation coefficient (r) for paired samples can be calculated from the t-statistic (t) and degrees of freedom (df) using the formula:\n\\[ r = \\frac{t}{\\sqrt{t^2 + df}} \\]\nWhere (t) is the t-statistic and (df) is the degrees of freedom.\nCalculating Effect Sizes for Paired Samples t-test:\n\nR:\n\n\n# We need to use the dataset with the dependent data\n# Regenerating the dataset to make sure we have the right one\nset.seed(123)\nn <- 30\nbefore_update <- rnorm(n, mean = 70, sd = 10)\nafter_update <- rnorm(n, mean = 75, sd = 10)\nstudydata_paired <- data.frame(\n  user_id = 1:n,\n  before_update = before_update,\n  after_update = after_update\n)\n\n\n# Calculate the mean difference and standard deviation of the differences\nmean_diff <- mean(studydata_paired$before_update - studydata_paired$after_update)\nsd_diff <- sd(studydata_paired$before_update - studydata_paired$after_update)\n\n# Calculate Cohen's d\ncohen_d_paired <- mean_diff / sd_diff\n\ncat(\"Cohen's d = \",cohen_d_paired)\n\nCohen's d =  -0.5238355\n\n\nTo calculate the correlation coefficient (r):\n\n# Perform the paired t-test\nt_test <- t.test(studydata_paired$before_update, studydata_paired$after_update, paired = TRUE)\n\n# Extract values from object of test output\nt_statistic <- t_test$statistic %>% unname\ndegfreedom <- t_test$parameter %>% unname\n\n# Calculate the correlation coefficient\nr_value <- t_statistic / sqrt(t_statistic^2 + degfreedom)\n\ncat(\"r = \",r_value)\n\nr =  -0.4702152\n\n\n\nStata:\n\n\n* Make sure the appropriate dataset is loaded (e.g. use DATASET_NAME)\n* Perform the paired t-test\nttest before_update == after_update\n\n* Calculate Cohen s d manually\ngen mean_diff = r(mu_1) - r(mu_2)\ngen sd_diff = sqrt(r(sd_1)^2 + r(sd_2)^2 - 2 * r(sd_1) * r(sd_2) * r(rho))\ngen cohen_d_paired = mean_diff / sd_diff\ndisplay cohen_d_paired\n\nTo calculate the correlation coefficient (r):\n\n* Calculate correlation coefficient\ngen r_value = r(t) / sqrt(r(t)^2 + r(df))\ndisplay r_value\n\n\n\n\n\n\n\n\n\nEffect Size\nRequired Information\nHow to Find It in Articles\n\n\n\n\nCohen’s d (paired)\nMean difference, standard deviation of differences\nLook for mean differences and standard deviations of the differences between conditions. These are usually reported in the results section.\n\n\nCorrelation coefficient (r)\nt-statistic, degrees of freedom\nThe t-statistic and degrees of freedom are typically found in the results section, often in a table summarizing the t-test results.\n\n\n\n\n\n3.8.3 Effect Size Magnitude Interpretation Guidelines\nThese are common guidelines used to interpret the magnitude of effect sizes:\n\n\n\n\n\n\n\n\n\nEffect Size Measure\nSmall\nMedium\nLarge\n\n\n\n\nCohen’s d (Independent or Paired)\n0.2\n0.5\n0.8\n\n\nCorrelation coefficient (r)\n0.1\n0.3\n0.5\n\n\n\n\n\n3.8.4 Differences Between Paired and Independent Effect Size Calculations\n\nIndependent Samples t-test: Cohen’s d is calculated using the pooled standard deviation of the two independent groups. The formula assumes that the two groups are independent and do not share any subjects.\nPaired Samples t-test: Cohen’s d for paired samples is calculated using the standard deviation of the differences between paired observations. This accounts for the fact that the same subjects are measured twice, and thus the observations are not independent.\n\nThe correlation coefficient (r) in paired samples t-tests can also provide insight into the strength of the relationship between the two sets of observations, which is not applicable in independent samples designs."
  },
  {
    "objectID": "ttest.html#r-vs.-stata-commands",
    "href": "ttest.html#r-vs.-stata-commands",
    "title": "3  t-test",
    "section": "3.9 R vs. Stata Commands",
    "text": "3.9 R vs. Stata Commands\n\n3.9.1 Statistical Analysis Commands\n\n\n\n\n\n\n\n\nStep\nR Command\nStata Command\n\n\n\n\nDescriptive Statistics\nsummary(studydata$satisfaction)\nsummarize satisfaction\n\n\nBox Plot\nggplot(studydata, aes(x = group, y = satisfaction)) + geom_boxplot()\ngraph box satisfaction, over(group)\n\n\nT-Test (Independent)\nt.test(satisfaction ~ group, data = studydata, var.equal = TRUE)\nttest satisfaction, by(group)\n\n\nT-Test (Paired)\nt.test(studydata$before_update, studydata$after_update, paired = TRUE)\nttest before_update == after_update\n\n\n\n\n\n3.9.2 Data Simulation Commands\n\n\n\n\n\n\n\n\nStep\nR Command\nStata Command\n\n\n\n\nGenerate Data\nrnorm(n, mean, sd)\nrnormal(mean, sd)\n\n\nSave Data\nwrite.csv(studydata, \"satisfaction_data.csv\")\nsave satisfaction_data.dta, replace\n\n\n\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum Associates."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum Associates.\n\n\nCox, Nicholas J. 2005. “A Brief History of Stata on Its 20th\nAnniversary.” The Stata Journal: Promoting Communications on\nStatistics and Stata 5 (1): 2–18. https://doi.org/10.1177/1536867x0500500102.\n\n\nPeng, Roger D. 2016. R Programming for Data Science. Leanpub\nVictoria, BC, Canada."
  }
]