# t-test

```{r echo=FALSE, message = FALSE, error=FALSE}
library(Statamarkdown) # to run Stata commands in an R environment
stataexe <- "C:/Program Files/Stata18/StataBE-64.exe"
knitr::opts_chunk$set(engine.path=list(stata=stataexe))
```

## Brief Explanation

The t-test, proposed by William Sealy Gosset under the pseudonym "Student" in 1908 [@87464e34-37bc-3288-807c-e421e5a0d7a6] [^ttest-1] [@ziliak2008a] [^ttest-2],is used to determine if there is a significant difference between the means of two groups. The t-test is applicable in various scenarios, including both small and large sample sizes, particularly when the population variance is unknown. The t-test assumes that the data is approximately normally distributed.

[^ttest-1]: Student. (1908). The Probable Error of a Mean. *Biometrika*, *6*(1), 1–25. https://doi.org/10.2307/2331554

[^ttest-2]: Ziliak, Stephen T. 2008. "Retrospectives: Guinnessometrics: The Economic Foundation of "Student's" *t*." *Journal of Economic Perspectives*, 22 (4): 199–216**.**

There are two main types of t-tests commonly used:

-   **Independent Samples t-test**: Compares the means of two independent groups (between-subjects design).

-   **Paired Samples t-test**: Compares means from the same group at different times or under different conditions (within-subjects design).

### Understanding Independent and Dependent Data

-   **Independent data**: In an independent samples t-test, the data from the two groups are independent, meaning that there is no inherent relationship between the observations in one group and the observations in the other group. This is typical of a **between-subjects design**, where different participants are assigned to different conditions.

-   **Dependent data**: In a paired samples t-test, the data are dependent, meaning that each observation in one condition is paired with an observation in another condition. This is typical of a **within-subjects design**, where the same participants are measured under different conditions or at different times.

## Research Scenario

Imagine a tech company, "BeautifulWeb," which has recently launched a new interface design for its flagship software. The company is keen to understand whether this new design truly enhances user satisfaction compared to the old design. The company has two different research questions:

1.  **Independent Samples Scenario**: BeautifulWeb randomly assigns users to use either the old or the new design and then measures their satisfaction. The research question is: *Does the new interface design improve user satisfaction compared to the old design?*

2.  **Paired Samples Scenario**: BeautifulWeb asks the same users to use both the old and new designs at different times and then measures their satisfaction. The research question is: *Does user satisfaction improve after using the new interface design compared to the old design?*

## Independent Samples t-test

### Research Question

Does the new interface design improve user satisfaction compared to the old design?

**Hypotheses**

-   **Null Hypothesis (H₀):** There is no significant difference in user satisfaction scores between the old and new interface designs.

-   **Alternative Hypothesis (H₁):** Users report higher satisfaction scores with the new interface design compared to the old design.

## Technical Requirements

Before we start, we need to prepare R and Stata (but mostly R, really) for the specific analysis we are focusing on. This implies loading any required packages typically used for these analyses, or ones that are useful to process the data (cleaning, shapng, tidying, transforming, etc.).

### Stata

No specific Stata libraries need to be installed. All that you need should be available by default in Stata.

### R

```{r message=FALSE, error=FALSE}
# Load the necessary packages 
library(tidyverse) # used for data manipulation and visualization
library(ggthemes) # adds new plotting themes for data visualization
library(cowplot) # nice plot theme and makes it easier to adjust plot text features
library(effsize) # for effect size calculations

# to install any missing packages go to the Terminal and run the command: install.packages("PACKAGE_NAME")
```

## Independent Samples t-test

### Simulate Data

Instead of relying on existng datasets with real data, we will generate our own data sets with fake data. These data were generated to reflect the research scenario in this chapter. This also means that these data are not real and therefore any conclusions drawn from the analyses are invalid, and serve only for educational purposes.

By generating a fake dataset, you are also learning how to do Data Simulation, something that might come in handy in the future if you need to conduct statistical power analyses, or create datasets to provide examples of your analyses in help-seeking contexts without having to share any sensitive real data (e.g., when asking for help with your code in platforms such as https://stats.stackexchange.com/).

#### Stata

```{stata}
clear
set seed 123
set obs 60
gen group = cond(_n <= 30, "Old Design", "New Design")
gen satisfaction = rnormal(70 + 5 * (group == "New Design"), 10)
save satisfaction_data_independent.dta, replace
```

#### R

```{r}
set.seed(123)
n <- 30
old_design <- rnorm(n, mean = 70, sd = 10)
new_design <- rnorm(n, mean = 75, sd = 10)
studydata <- data.frame(
  group = rep(c("Old Design", "New Design"), each = n),
  satisfaction = c(old_design, new_design)
)
write.csv(studydata, "satisfaction_data_independent.csv", row.names = FALSE)
```

### Descriptives and data visualization

#### Stata

```{stata}
use satisfaction_data_independent.dta
summarize satisfaction
graph box satisfaction, over(group)
```

![](images/ttest_boxplot.png)

#### R

```{r}
# Import data
studydata <- read.csv("satisfaction_data_independent.csv") 

# Summary of data
summary(studydata$satisfaction)
```

```{r}
ggplot(studydata, aes(x = group, y = satisfaction, fill = group)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, outlier.size = 2) +
  scale_fill_brewer(palette = "Set3") +
  theme_cowplot() +
  labs(
    title = "Satisfaction Scores by Design Group",
    x = "Design Group",
    y = "Satisfaction Score"
  )
```

### Performing the t-test

**Important Note About R's t-test**

By default, R uses **Welch's t-test**, which does not assume equal variances between the groups. This is often a more robust approach, but if you want to match the classical t-test calculation (assuming equal variances), you need to specify `var.equal = TRUE` in the `t.test()` function. The code provided here uses the classical approach to ensure it matches the Stata output.

#### Stata

```{stata}
use satisfaction_data_independent.dta
ttest satisfaction, by(group)
```

#### R

```{r}
t.test(satisfaction ~ group, data = studydata, var.equal = TRUE)
```

### Interpretation

-   **P-value**: Indicates whether the difference in means is statistically significant.
-   **Confidence Interval**: By default, this is a 95% confidence interval, but you can adjust this with the `conf.level` parameter in the `t.test()` function.

## Paired Samples t-test

### Research Question

**Research Question:** Does user satisfaction improve after using the new interface design compared to the old design?

**Hypothesis**

-   **Null Hypothesis (H₀):** There is no significant difference in user satisfaction scores before and after using the new interface design.

-   **Alternative Hypothesis (H₁):** Users report higher satisfaction scores after using the new interface design compared to before.

### Simulate Data

#### Stata

```{stata}
clear
set seed 123
set obs 30
gen user_id = _n
gen before_update = rnormal(70, 10)
gen after_update = rnormal(75, 10)
save satisfaction_data_paired.dta, replace
```

#### R

```{r}
set.seed(123)
n <- 30
before_update <- rnorm(n, mean = 70, sd = 10)
after_update <- rnorm(n, mean = 75, sd = 10)
studydata <- data.frame(
  user_id = 1:n,
  before_update = before_update,
  after_update = after_update
)
write.csv(studydata, "satisfaction_data_paired.csv", row.names = FALSE)
```

### Descriptives and data visualization

#### Stata

```{stata}
use satisfaction_data_paired.dta
summarize before_update after_update
graph box before_update after_update, title("Satisfaction Scores Before and After Update") ytitle("Satisfaction Score") b1title("Update Status") nooutsides
```

#### ![](images/ttest_paired_boxplot.png)

#### R

```{r}
studydata <- read.csv("satisfaction_data_paired.csv")
summary(studydata[, c("before_update", "after_update")])

# Reshape the data to long format
studydata_long <- studydata %>%
  pivot_longer(cols = c("before_update", "after_update"), 
               names_to = "Condition", 
               values_to = "Satisfaction")

# Create the boxplot
p <- ggplot(studydata_long, aes(x = Condition, y = Satisfaction, fill = Condition)) +
  geom_boxplot(alpha = 0.7) +
  theme_cowplot() +
  labs(title = "Satisfaction Scores Before and After Update", 
       x = "Condition", 
       y = "Satisfaction Score") +
  scale_fill_manual(values = c("blue", "green"))

p
```

### Performing the t-test

#### Stata

```{stata}
use satisfaction_data_paired.dta
ttest before_update == after_update
```

#### R

```{r}
studydata <- read.csv("satisfaction_data_paired.csv")
t.test(studydata$before_update, studydata$after_update, paired = TRUE)
```

### Interpretation

-   **P-value**: Indicates whether the difference in means is statistically significant.
-   **Confidence Interval**: It can be adjusted from the default 95% to another level if needed (but simply use 95% in case you are in doubt).

## Explanation of Relevant Terms

| **Term**                | **Definition**                                                                                                                                                                                                                                          | **Common Misconception**                                                                                     |
|-------------------|----------------------------------|-------------------|
| **P-value**             | The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is true.                                                                                                  | The p-value is the probability that the null hypothesis is true.                                             |
| **Confidence Interval** | A range of values, derived from the sample data, that is believed to contain the true parameter value with a certain probability. The most common level of confidence is 95%, but this can be adjusted (e.g., to 90% or 99%) depending on the analysis. | A 95% confidence interval means there is a 95% probability that the true parameter lies within the interval. |
| **T-statistic**         | A ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. The degrees of freedom (df) are the number of independent values that can vary in an analysis without breaking any constraints.       | The t-statistic directly tells us the probability of the null hypothesis being true.                         |

## Interpretation Questions

1.  **What does a significant p-value indicate in the context of this t-test?**

<button onclick="toggleSolution(&#39;sol1&#39;)">

Show/Hide Solution 1

</button>

::: {#sol1 style="display:none;"}
**Solution 1:** A significant p-value indicates that the observed data is unlikely under the null hypothesis. This suggests that there is evidence against the null hypothesis, implying a statistically significant difference between the satisfaction scores of the two groups (for independent samples) or before and after the update (for paired samples). However, it does not measure the probability that the null hypothesis is true or false.
:::

2.  **How would you interpret the confidence interval in this analysis?**

<button onclick="toggleSolution(&#39;sol2&#39;)">

Show/Hide Solution 2

</button>

::: {#sol2 style="display:none;"}
**Solution 2:** The confidence interval provides a range of values that, based on the sample data, is likely to contain the true mean difference. If we were to repeat the experiment many times, we would expect a specified proportion (e.g., 95%) of these intervals to contain the true mean difference. It does not mean that there is a 95% probability that the true mean difference lies within this specific interval. The most common confidence level is 95%, but it can be adjusted depending on the requirements of the analysis.
:::

```{=html}
<script>
function toggleSolution(id) {
  var element = document.getElementById(id);
  if (element.style.display === "none") {
    element.style.display = "block";
  } else {
    element.style.display = "none";
  }
}
</script>
```
## Effect Sizes for t-tests

Effect sizes are a crucial part of reporting t-test results because they provide information on the magnitude of the difference between groups or conditions, beyond just the statistical significance. Below, we discuss the common effect sizes to report for Independent Samples t-tests and Paired Samples t-tests. The most popular effect size to report for t-tests is Cohen's d.

The interpretation guidelines for effect sizes such as Cohen's d and the correlation coefficient (r) are based on widely accepted conventions in the field of psychology and social sciences. These conventions were originally proposed by Jacob Cohen in his foundational work on statistical power analysis.

Cautionary notes from Jacob Cohen regarding his proposed guidelines on effect sizes can be found in his book *Statistical Power Analysis for the Behavioral Sciences* [@cohen1988statistical] [^ttest-3] and subsequent paper *A Power Primer* [@cohen1992power] [^ttest-4]. Cohen emphasized that the guidelines he provided for interpreting effect sizes (i.e., small, medium, and large) were meant to be rough, arbitrary conventions rather than rigid rules. He also proposed that a medium effect size should be visible without the need for statistical tools, though this expectation may not be practical across the diverse fields of research that apply these guidelines. Additionally, Cohen emphasized that these benchmarks are intended for use only when more precise estimates specific to the particular area of study are not available.

[^ttest-3]: Cohen, Jacob. 1988. *Statistical Power Analysis for the BehavioralSciences*. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum Associates.

[^ttest-4]: Cohen, J. (1992). A power primer. *Psycholigcal Bulletin*, 112 (1), 155–159.

**Calculating Cohen's d for Independent Samples t-test:**

-   **R:**

To calculate Cohen's *d* for an Independent Samples t-test in R, you can use the `effsize` package, which provides a reliable implementation.

```{r}
# We need to use the dataset with the independent groups
# Regenerating the dataset to make sure we have the right one
set.seed(123)
n <- 30
old_design <- rnorm(n, mean = 70, sd = 10)
new_design <- rnorm(n, mean = 75, sd = 10)
studydata_independent <- data.frame(
  group = rep(c("Old Design", "New Design"), each = n),
  satisfaction = c(old_design, new_design)
)
```

```{r}
# Note: Use the dataset with independent samples: studydata_long

# Compute cohen's d
cohen_d <- cohen.d(satisfaction ~ group, data = studydata_independent, pooled = TRUE)
cat(" Cohen's d: ", cohen_d$estimate)
```

-   **Stata:**

Stata doesn’t have a built-in command to directly calculate Cohen’s *d*, but it can be calculated using the following steps:

```{stata}
* Make sure the appropriate dataset is loaded (e.g. use DATASET_NAME)
* Assuming the data is already loaded in Stata
ttest satisfaction, by(group)

* Calculate manually
gen pooled_sd = sqrt(((r(sd_1)^2) * (r(N_1) - 1) + (r(sd_2)^2) * (r(N_2) - 1)) / (r(N_1) + r(N_2) - 2))
gen cohen_d = (r(mu_1) - r(mu_2)) / pooled_sd
display cohen_d
```

| **Effect Size** | **Required Information**                                | **How to Find It in Articles**                                                                                                                                          |
|-------------------|-----------------------|------------------------------|
| **Cohen's d**   | Means of both groups, standard deviations, sample sizes | Look for mean differences, standard deviations, and sample sizes in the results section of articles. Typically, these are presented in tables or described in the text. |

**Calculating Effect Sizes for Paired Samples t-test:**

-   **R:**

```{r}
# We need to use the dataset with the dependent data
# Regenerating the dataset to make sure we have the right one
set.seed(123)
n <- 30
before_update <- rnorm(n, mean = 70, sd = 10)
after_update <- rnorm(n, mean = 75, sd = 10)
studydata_paired <- data.frame(
  user_id = 1:n,
  before_update = before_update,
  after_update = after_update
)
```

```{r}
# Calculate the mean difference and standard deviation of the differences
mean_diff <- mean(studydata_paired$before_update - studydata_paired$after_update)
sd_diff <- sd(studydata_paired$before_update - studydata_paired$after_update)

# Calculate Cohen's d
cohen_d_paired <- mean_diff / sd_diff

cat("Cohen's d = ",cohen_d_paired)
```

To calculate the correlation coefficient (r):

```{r}
# Perform the paired t-test
t_test <- t.test(studydata_paired$before_update, studydata_paired$after_update, paired = TRUE)

# Extract values from object of test output
t_statistic <- t_test$statistic %>% unname
degfreedom <- t_test$parameter %>% unname

# Calculate the correlation coefficient
r_value <- t_statistic / sqrt(t_statistic^2 + degfreedom)

cat("r = ",r_value)
```

-   **Stata:**

```{stata}
* Make sure the appropriate dataset is loaded (e.g. use DATASET_NAME)
* Perform the paired t-test
ttest before_update == after_update

* Calculate Cohen s d manually
gen mean_diff = r(mu_1) - r(mu_2)
gen sd_diff = sqrt(r(sd_1)^2 + r(sd_2)^2 - 2 * r(sd_1) * r(sd_2) * r(rho))
gen cohen_d_paired = mean_diff / sd_diff
display cohen_d_paired
```

To calculate the correlation coefficient (r):

```{stata}
* Calculate correlation coefficient
gen r_value = r(t) / sqrt(r(t)^2 + r(df))
display r_value
```

| **Effect Size**                 | **Required Information**                           | **How to Find It in Articles**                                                                                                              |
|-------------------|-----------------------|------------------------------|
| **Cohen's d (paired)**          | Mean difference, standard deviation of differences | Look for mean differences and standard deviations of the differences between conditions. These are usually reported in the results section. |
| **Correlation coefficient (r)** | t-statistic, degrees of freedom                    | The t-statistic and degrees of freedom are typically found in the results section, often in a table summarizing the t-test results.         |

| **Effect Size Measure**               | **Small** | **Medium** | **Large** |
|---------------------------------------|-----------|------------|-----------|
| **Cohen's d (Independent or Paired)** | 0.2       | 0.5        | 0.8       |
| **Correlation coefficient (r)**       | 0.1       | 0.3        | 0.5       |

### Differences Between Paired and Independent Effect Size Calculations

-   **Independent Samples t-test**: Cohen's d is calculated using the pooled standard deviation of the two independent groups. The formula assumes that the two groups are independent and do not share any subjects.

-   **Paired Samples t-test**: Cohen's d for paired samples is calculated using the standard deviation of the differences between paired observations. This accounts for the fact that the same subjects are measured twice, and thus the observations are not independent.

The correlation coefficient (r) in paired samples t-tests can also provide insight into the strength of the relationship between the two sets of observations, which is not applicable in independent samples designs.

## R vs. Stata Commands

### Statistical Analysis Commands

| Step                   | R Command                                                                | Stata Command                         |
|-------------------|---------------------------------|--------------------|
| Descriptive Statistics | `summary(studydata$satisfaction)`                                        | `summarize satisfaction`              |
| Box Plot               | `ggplot(studydata, aes(x = group, y = satisfaction)) + geom_boxplot()`   | `graph box satisfaction, over(group)` |
| T-Test (Independent)   | `t.test(satisfaction ~ group, data = studydata, var.equal = TRUE)`       | `ttest satisfaction, by(group)`       |
| T-Test (Paired)        | `t.test(studydata$before_update, studydata$after_update, paired = TRUE)` | `ttest before_update == after_update` |

### Data Simulation Commands

| Step          | R Command                                       | Stata Command                         |
|-------------------|------------------------------|-----------------------|
| Generate Data | `rnorm(n, mean, sd)`                            | `rnormal(mean, sd)`                   |
| Save Data     | `write.csv(studydata, "satisfaction_data.csv")` | `save satisfaction_data.dta, replace` |
